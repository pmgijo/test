import "dotenv/config";
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";
import OpenAI from "openai";
import { ChromaClient } from "chromadb";
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

const openai = new OpenAI();
const chromaClient = new ChromaClient({ path: "http://localhost:8000" });
chromaClient.heartbeat();

const WEB_COLLECTION = "RESPONSIBLE1";

// Function to extract text from PDF
async function extractTextFromPdf(pdfPath) {
  const loader = new PDFLoader(pdfPath);
  const docs = await loader.load();
  const combinedText = docs.map(doc => doc.pageContent).join("\n");
  return combinedText; 
}

// Function to insert vector embeddings into ChromaDB
async function insertIntoVector({ embedding, url, body = "" }) {
  const collection = await chromaClient.getOrCreateCollection({
    name: WEB_COLLECTION,
  });
  collection.add({
    ids: [url],
    embeddings: [embedding],
    metadatas: [{body}],
    documents: [body],
  });
}

// Function to generate vector embeddings using OpenAI
async function generateVectorEmbeddings({ text }) {
  const embedding = await openai.embeddings.create({
    model: "text-embedding-ada-002",
    input: text,
    encoding_format: "float",
  });
  return embedding.data[0].embedding;
}

// Function to ingest the PDF content into ChromaDB
async function ingest(pdfPath = "") {
  const pdfText = await extractTextFromPdf(pdfPath);

  // Use LangChain TextSplitter to chunk the PDF text
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000, // Adjust chunk size to your needs
    chunkOverlap: 200, // Set overlap for chunking
  });

  // Split the extracted text into chunks
  const bodyChunks = await textSplitter.splitText(String(pdfText));
  console.log(bodyChunks);

  let id=0;
  // Iterate over the chunks and generate embeddings
  for (const chunk of bodyChunks) {
    const bodyEmbedding = await generateVectorEmbeddings({ text: chunk });
    await insertIntoVector({
      embedding: bodyEmbedding,
      url: `ID${id}`, // Storing the PDF path as the "url"
      body: chunk,
    });
    id++;
  }

  console.log(`ðŸ¶ PDF ${pdfPath} ingested`);
}

// Function to chat and get relevant context from ChromaDB
async function chat(question = "") {
  const questionEmbedding = await generateVectorEmbeddings({ text: question });
  
  const collection = await chromaClient.getOrCreateCollection({
    name: WEB_COLLECTION,
  });
  
  // Query the ChromaDB collection to get the most relevant data
  const collectionResult = await collection.query({
    nResults: 10, // Adjust the number of relevant results
    queryEmbeddings: questionEmbedding,
  });
  
  // Get the body content of the retrieved results
  const body = collectionResult.metadatas[0]
  .map((e) => e.body)
  .filter((e) => e.trim() !== "" || !!e);

  // Use OpenAI to generate a response based on the retrieved context
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "system",
        content:
          "You are an AI support assistant that provides answers based on the content of the PDF. Reply to the user based on the context provided.",
      },
      {
        role: "user",
        content: `
          Query: ${question}\n\n
          Retrieved Context: ${body}
        `,
      },
    ],
  });

  console.log(`ðŸ¦Š: ${response.choices[0].message.content}`);
}

// Example usage: Ingesting the PDF and asking a question
const pdfPath = "./Responsible-Lending.pdf";

// Ingest the PDF into ChromaDB (call this once)
// await ingest(pdfPath);

// Ask a question
await chat("What is ePayments?");
