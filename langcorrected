import dotenv from "dotenv";
dotenv.config();

import { TextLoader } from "langchain/document_loaders/fs/text";
import { CharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { AzureOpenAIEmbeddings } from "@langchain/openai";
import { Annotation } from "@langchain/langgraph";
import { createRetrieverTool } from "langchain/tools/retriever";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { HumanMessage } from "@langchain/core/messages";
import { START, END } from "@langchain/langgraph";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph } from "@langchain/langgraph";
import { pull } from "langchain/hub";
import { z } from "zod";

// Load and split documents
const loader = new TextLoader("./policy.txt");
const docs = await loader.load();
const splitter = new CharacterTextSplitter({ chunkSize: 200, chunkOverlap: 50 });
const documents = await splitter.splitDocuments(docs);

// Create an instance of AzureOpenAIEmbeddings
const embeddings = new AzureOpenAIEmbeddings({
  apiEndpoint: process.env.AZURE_OPENAI_ENDPOINT,
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  azureOpenAIApiInstanceName: "hb-genai-test",
  azureOpenAIApiDeploymentName: "text-embedding-ada-002",
  azureOpenAIApiVersion: "2023-05-15",
  model: "text-embedding-ada-002",
});

const vectorStore = await MemoryVectorStore.fromDocuments(documents, embeddings);
const retriever = vectorStore.asRetriever();

const GraphState = Annotation.Root({
  messages: Annotation({
    reducer: (x, y) => x.concat(y),
    default: () => [],
  }),
});

const tool = createRetrieverTool(retriever, {
  name: "retrieve_documents",
  description: "Search and return information based on the loaded documents.",
});

const tools = [tool];
const toolNode = new ToolNode(tools);

async function agent(state) {
  const { messages } = state;
  const llm = new ChatOpenAI({
    apiEndpoint: process.env.AZURE_OPENAI_ENDPOINT,
    apiKey: process.env.AZURE_OPENAI_API_KEY,
    azureOpenAIApiInstanceName: "hb-genai-test",
    azureOpenAIApiDeploymentName: "gpt-4o",
    azureOpenAIApiVersion: "2024-08-01-preview",
    model: "gpt-4o",
  });
  const completion = await llm.invoke(messages).bindTools(tools);
  return { messages: [completion] };
}

async function gradeDocuments(state) {
  const { messages } = state;
  const tool = {
    name: "give_relevance_score",
    description: "Give a relevance score to the retrieved documents.",
    schema: z.object({
      binaryScore: z.string().describe("Relevance score 'yes' or 'no'"),
    }),
  };

  const prompt = ChatPromptTemplate.fromTemplate(
    `You are a grader assessing relevance of retrieved docs to a user question.
    Here are the retrieved docs:
    \n ------- \n
    {context} 
    \n ------- \n
    Here is the user question: {question}
    If the content of the docs are relevant to the user's question, score them as relevant.
    Give a binary score 'yes' or 'no' to indicate whether the docs are relevant to the question.`
  );

  const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0 }).bindTools([
    tool,
  ]);

  const chain = prompt.pipe(model);
  const lastMessage = messages[messages.length - 1];

  const score = await chain.invoke({
    question: messages[0].content,
    context: lastMessage.content,
  });

  return {
    messages: [score],
  };
}

function shouldRetrieve(state) {
  const { messages } = state;
  const lastMessage = messages[messages.length - 1];

  if (
    "tool_calls" in lastMessage &&
    Array.isArray(lastMessage.tool_calls) &&
    lastMessage.tool_calls.length
  ) {
    return "retrieve";
  }
  return END;
}

function checkRelevance(state) {
  const { messages } = state;
  const lastMessage = messages[messages.length - 1];
  if (!("tool_calls" in lastMessage)) {
    throw new Error(
      "The 'checkRelevance' node requires the most recent message to contain tool calls."
    );
  }

  const toolCalls = lastMessage.tool_calls;
  if (toolCalls && toolCalls[0].args.binaryScore === "yes") {
    return "yes";
  }
  return "no";
}

async function rewrite(state) {
  const { messages } = state;
  const question = messages[0].content;

  const prompt = ChatPromptTemplate.fromTemplate(
    `Look at the input and try to reason about the underlying semantic intent / meaning.
    Here is the initial question:
    \n ------- \n
    {question} 
    \n ------- \n
    Formulate an improved question:`
  );

  const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0 });
  const response = await prompt.pipe(model).invoke({ question });

  return {
    messages: [response],
  };
}

async function generate(state) {
  const { messages } = state;
  const question = messages[0].content;

  const lastToolMessage = messages
    .slice()
    .reverse()
    .find((msg) => msg._getType() === "tool");

  if (!lastToolMessage) {
    throw new Error("No tool message found in the conversation history.");
  }

  const docs = lastToolMessage.content;
  const prompt = await pull("rlm/rag-prompt");

  const llm = new ChatOpenAI({
    model: "gpt-4o",
    temperature: 0,
    streaming: true,
  });

  const ragChain = prompt.pipe(llm);
  const response = await ragChain.invoke({
    context: docs,
    question,
  });

  return {
    messages: [response],
  };
}

const workflow = new StateGraph(GraphState)
  .addNode("agent", agent)
  .addNode("retrieve", toolNode)
  .addNode("gradeDocuments", gradeDocuments)
  .addNode("rewrite", rewrite)
  .addNode("generate", generate);

workflow.addEdge(START, "agent");
workflow.addConditionalEdges("agent", shouldRetrieve);
workflow.addEdge("retrieve", "gradeDocuments");
workflow.addConditionalEdges("gradeDocuments", checkRelevance, {
  yes: "generate",
  no: "rewrite",
});
workflow.addEdge("generate", END);
workflow.addEdge("rewrite", "agent");

const app = workflow.compile();

const inputs = {
  messages: [new HumanMessage("What was his contribution?")],
};

let finalState;
for await (const output of await app.stream(inputs)) {
  for (const [key, value] of Object.entries(output)) {
    const lastMsg = value.messages[value.messages.length - 1];
    console.log(`Output from node: '${key}'`);
    console.dir(
      {
        type: lastMsg._getType(),
        content: lastMsg.content,
        tool_calls: lastMsg.tool_calls,
      },
      { depth: null }
    );
    finalState = value;
  }
}

console.log(JSON.stringify(finalState, null, 2));
